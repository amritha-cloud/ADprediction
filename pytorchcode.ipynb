{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H44J1efIc8rG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import imblearn\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "from skimage.exposure import equalize_adapthist as eq_hist\n",
        "from keras.utils.data_utils import Sequence\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend\n",
        "from keras.models import load_model\n",
        "import math\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Flatten, BatchNormalization, Dropout, Convolution2D, Conv2D, MaxPooling2D, LeakyReLU\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Settign the dir path \n",
        "TRAIN_DIR = '../input/alzheimers-classification-dataset/dataset/train/'\n",
        "TEST_DIR = '../input/alzheimers-classification-dataset/dataset/test/'\n",
        "IMAGE_SIZE = 176\n",
        "CLASSES = [\n",
        "    'NonDemented',\n",
        "    'VeryMildDemented',\n",
        "    'MildDemented',\n",
        "    'ModerateDemented',\n",
        "]\n",
        "train_images = {}\n",
        "test_images = {}\n",
        "\n",
        "for klass in CLASSES:\n",
        "    train_images[klass] = []\n",
        "    test_images[klass] = []\n",
        "\n",
        "for klass in CLASSES:\n",
        "    for image in os.listdir(TRAIN_DIR + klass):\n",
        "        im = Image.open(TRAIN_DIR + klass + '/' + image).convert('L')\n",
        "        train_images[klass].append(im.resize((IMAGE_SIZE, IMAGE_SIZE)))\n",
        "        \n",
        "    for image in os.listdir(TEST_DIR + klass):\n",
        "        im = Image.open(TEST_DIR + klass + '/' + image).convert('L')\n",
        "        test_images[klass].append(im.resize((IMAGE_SIZE, IMAGE_SIZE)))"
      ],
      "metadata": {
        "id": "tE8jsLgadCW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(1, figsize=(20, 4))\n",
        "i = 1\n",
        "for klass in CLASSES:\n",
        "    avg_pic = np.zeros((IMAGE_SIZE, IMAGE_SIZE))\n",
        "    for pic in train_images[klass] + test_images[klass]:\n",
        "        avg_pic += np.array(pic)\n",
        "    avg_pic = avg_pic / (len(train_images[klass]) + len(test_images[klass]))\n",
        "    plt.subplot(1, 4, i)\n",
        "    i += 1\n",
        "    plt.imshow(avg_pic, cmap='gray')\n",
        "    plt.xlabel('%s average\\ntotally %d/%d pics' % (klass, len(train_images[klass]), len(test_images[klass])))\n",
        "plt.show()\n",
        "\n",
        "class_weight = {}\n",
        "max_classes = len(test_images['NonDemented']) / 100\n",
        "for i, klass in enumerate(CLASSES):\n",
        "    class_weight[i] = max_classes / len(test_images[klass])\n",
        "class_weight\n",
        "{0: 0.01,\n",
        " 1: 0.014285714285714287,\n",
        " 2: 0.03575418994413408,\n",
        " 3: 0.5333333333333333}"
      ],
      "metadata": {
        "id": "nJFlVuXsdFOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def equalize(im):\n",
        "    return eq_hist(np.array(im), clip_limit=0.03)\n",
        "\n",
        "train_data = np.array([equalize(i) for i in train_images[CLASSES[0]]])\n",
        "train_labels = np.zeros((len(train_images[CLASSES[0]]), 1))\n",
        "for ind, klass in enumerate(CLASSES[1:], start=1):\n",
        "    klass_arr = np.array([equalize(i) for i in train_images[klass]])\n",
        "    train_data = np.concatenate([train_data, klass_arr], axis=0)\n",
        "    \n",
        "    labels_arr = np.ones((len(train_images[klass]), 1)) * ind\n",
        "    train_labels = np.concatenate([train_labels, labels_arr], axis=0)\n",
        "\n",
        "test_data = np.array([equalize(i) for i in test_images[CLASSES[0]]])\n",
        "test_labels = np.zeros((len(test_images[CLASSES[0]]), 1))\n",
        "for ind, klass in enumerate(CLASSES[1:], start=1):\n",
        "    klass_arr = np.array([equalize(i) for i in test_images[klass]])\n",
        "    test_data = np.concatenate([test_data, klass_arr], axis=0)\n",
        "    \n",
        "    labels_arr = np.ones((len(test_images[klass]), 1)) * ind\n",
        "    test_labels = np.concatenate([test_labels, labels_arr], axis=0)\n",
        "\n",
        "    \n",
        "train_data = train_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "test_data = test_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels)\n",
        "test_labels = np_utils.to_categorical(test_labels)\n",
        "\n",
        "train_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n",
        "#((5121, 176, 176, 1), (5121, 4), (1279, 176, 176, 1), (1279, 4))\n",
        "\n",
        "\n",
        "#using random permutation\n",
        "p = np.random.permutation(train_data.shape[0])\n",
        "train_data = train_data[p]\n",
        "train_labels = train_labels[p]\n",
        "\n",
        "#using random oversampling to compensate for lack of homegeneity in classes\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "train_ros_data, train_ros_labels = ros.fit_resample(train_data.reshape((-1, IMAGE_SIZE * IMAGE_SIZE)), train_labels)\n",
        "test_ros_data, test_ros_labels = ros.fit_resample(test_data.reshape((-1, IMAGE_SIZE * IMAGE_SIZE)), test_labels)\n",
        "\n",
        "train_ros_data = train_ros_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "test_ros_data = test_ros_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\n",
        "\n",
        "train_ros_data.shape, train_ros_labels.shape, test_ros_data.shape, test_ros_labels.shape\n",
        "#((10240, 176, 176, 1), (10240, 4), (2560, 176, 176, 1), (2560, 4))\n",
        "\n",
        "# this callback applies cosine annealing, saves snapshots and allows to load them\n",
        "class SnapshotEnsemble(Callback):\n",
        "    \n",
        "    __snapshot_name_fmt = \"snapshot_%d.hdf5\"\n",
        "    \n",
        "    def __init__(self, n_models, n_epochs_per_model, lr_max, verbose=1):\n",
        "        \"\"\"\n",
        "        n_models -- quantity of models (snapshots)\n",
        "        n_epochs_per_model -- quantity of epoch for every model (snapshot)\n",
        "        lr_max -- maximum learning rate (snapshot starter)\n",
        "        \"\"\"\n",
        "        self.n_epochs_per_model = n_epochs_per_model\n",
        "        self.n_models = n_models\n",
        "        self.n_epochs_total = self.n_models * self.n_epochs_per_model\n",
        "        self.lr_max = lr_max\n",
        "        self.verbose = verbose\n",
        "        self.lrs = []\n",
        " \n",
        "    # calculate learning rate for epoch\n",
        "    def cosine_annealing(self, epoch):\n",
        "        cos_inner = (math.pi * (epoch % self.n_epochs_per_model)) / self.n_epochs_per_model\n",
        "        return self.lr_max / 2 * (math.cos(cos_inner) + 1)\n",
        "\n",
        "    # when epoch begins update learning rate\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        # update learning rate\n",
        "        lr = self.cosine_annealing(epoch)\n",
        "        backend.set_value(self.model.optimizer.lr, lr)\n",
        "        # log value\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "    # when epoch ends check if there is a need to save a snapshot\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if (epoch + 1) % self.n_epochs_per_model == 0:\n",
        "            # save model to file\n",
        "            filename = self.__snapshot_name_fmt % ((epoch + 1) // self.n_epochs_per_model)\n",
        "            self.model.save(filename)\n",
        "            if self.verbose:\n",
        "                print('Epoch %d: snapshot saved to %s' % (epoch, filename))\n",
        "                \n",
        "    # load all snapshots after training\n",
        "    def load_ensemble(self):\n",
        "        models = []\n",
        "        for i in range(self.n_models):\n",
        "            models.append(load_model(self.__snapshot_name_fmt % (i + 1)))\n",
        "        return models\n",
        "\n",
        "\n",
        "#Image augmentation\n",
        "imagegen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=15,\n",
        "    height_shift_range=15,\n",
        "    zoom_range=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "3LZivwT4dHuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE,IMAGE_SIZE,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(.0001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(len(CLASSES), activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "hDHVSQr9dUWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "se_callback = SnapshotEnsemble(n_models=1, n_epochs_per_model=50, lr_max=.02)\n",
        "history = model.fit_generator(\n",
        "            imagegen.flow(train_ros_data, train_ros_labels, \n",
        "            batch_size=32), \n",
        "            steps_per_epoch=len(train_ros_data) / 32,\n",
        "            epochs= se_callback.n_epochs_total,\n",
        "            verbose=1,\n",
        "            callbacks=[se_callback],\n",
        "            validation_data=(test_ros_data, test_ros_labels)\n",
        ")"
      ],
      "metadata": {
        "id": "lNZu9mhedWqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"alzheimers50.h5\")"
      ],
      "metadata": {
        "id": "-fvTMdRZdaa4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}